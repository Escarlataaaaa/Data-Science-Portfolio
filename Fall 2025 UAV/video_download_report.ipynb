{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182ccd16-70d7-4ab4-aafd-d1aa2151c517",
   "metadata": {},
   "source": [
    "# Automated Video Retrieval from ROS Bag Files via Batch \n",
    "\n",
    "This notebook documents the full workflow for automatically retrieving short video clips\n",
    "embedded in ROS bag files from a Batch API. It explains the motivation, design decisions,\n",
    "implementation details, and challenges encountered during development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06865d4a-7f63-4180-8f3a-4ccbd4c33f08",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "The goal of this project was to build a reliable and scalable pipeline to automatically\n",
    "download short video clips embedded in ROS bag files using the Batch API.\n",
    "\n",
    "Each clip:\n",
    "- Covers **30 seconds** of video\n",
    "- Is associated with a **specific vehicle** and **camera**\n",
    "- Is requested using precise **UTC timestamps**\n",
    "- Is archived in a **date-based folder structure**\n",
    "\n",
    "The final pipeline supports:\n",
    "- Multiple vehicles (e.g. `mallory`, `mav`, `megalodon`, `metatron`, `morizo`)\n",
    "- Multiple camera views (e.g. front-center, left-center, right-center)\n",
    "- Long time windows (up to full-day retrievals)\n",
    "- Execution on **Savio HPC** to improve performance and scalability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd0b2-f56d-461b-af6b-233d6405c8dc",
   "metadata": {},
   "source": [
    "## 2. Task Description\n",
    "\n",
    "The task consists of the following high-level steps:\n",
    "\n",
    "1. Authenticate with the Batch API using a manually retrieved token\n",
    "2. Request video clips in **30-second windows** for a given vehicle and camera\n",
    "3. Poll the API until each clip is ready\n",
    "4. Download the resulting ROS bag file\n",
    "5. Organize clips into vehicle-specific subfolders\n",
    "6. Enforce Savio storage constraints (≤ 10 GB per subfolder)\n",
    "7. Archive downloaded clips into date-based folders for long-term storage\n",
    "\n",
    "The pipeline was first prototyped locally and later migrated to Savio for faster execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2548e7-12b1-4168-b2eb-3beccb08c58b",
   "metadata": {},
   "source": [
    "## 3. Environment Setup and Constraints\n",
    "\n",
    "### Local Development\n",
    "- Initial testing and debugging were done on a local machine\n",
    "- Performance was limited due to network speed and I/O constraints\n",
    "\n",
    "### Savio HPC\n",
    "- Migration to Savio Jupyter (HTC partition, single-core jobs)\n",
    "- Significantly faster API requests and downloads\n",
    "- **Storage constraint:** ~20 GB total per workspace\n",
    "- **Download constraint:** subfolders capped at **10 GB**\n",
    "\n",
    "To work within these constraints:\n",
    "- Downloaded files were periodically transferred to an external SSD\n",
    "- Local Savio storage was cleaned after each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9e92cb-df49-4d6a-97b1-6d0de62f208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from zoneinfo import ZoneInfo\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384f2866-3937-4543-8dc8-2cf112cdc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE = \"https://fleet-batch.api.maymobility.com\"\n",
    "VIDEO_URL = f\"{API_BASE}/v1/video\"\n",
    "DL_URL = f\"{API_BASE}/v1/download\"\n",
    "VEHICLES = [\"metatron\"] \n",
    "CAMERA = \"front-center\" \n",
    "DURATION_SECONDS = 30  \n",
    "STEP_SECONDS = 30  \n",
    "THROTTLE_SEC = 0.2\n",
    "DAY_TZ = datetime.now(timezone.utc)\n",
    "SKIP_ON_404_SEC = 300\n",
    "LOOKBACK_S = 5 * 24 * 3600     \n",
    "MAX_FILES_PER_VEHICLE = None \n",
    "MIN_VALID_BYTES = 1_000_000\n",
    "MAX_OUTPUT_BYTES = 10 * 1024**3\n",
    "BUCKET_PREFIX = \"download\"\n",
    "\n",
    "OUTPUT_DIR = pathlib.Path(\"downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f6f0d-cef2-4f10-90f6-ee5cd35c8768",
   "metadata": {},
   "source": [
    "## 4. Authentication and API Access\n",
    "\n",
    "All API requests require a valid access token.\n",
    "\n",
    "Key characteristics:\n",
    "- Tokens expire after **24 hours**\n",
    "- Tokens are manually retrieved and updated\n",
    "- Token is passed via the HTTP `Authorization` header\n",
    "\n",
    "This design choice ensures security but requires manual renewal during long-running jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adba5217-e780-44c3-a33a-faf888917908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update every day before running\n",
    "token = \"eyJraWQiOiJvcXJNWDRyVXJ2V21lbVhENDRFRVwvWFF0YWFaY3dWbFZzM3M4RElKekREcz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiI2NzNzZ3V2ZW05ZnJxdnZmMWg3bGI4YjY5NCIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoiYmF0Y2gtdGVsZW1ldHJ5XC9jY3RhIiwiYXV0aF90aW1lIjoxNzYzNDI5MzY5LCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9oRTI5U1hRVXYiLCJleHAiOjE3NjM1MTU3NjksImlhdCI6MTc2MzQyOTM2OSwidmVyc2lvbiI6MiwianRpIjoiYjE5YmJkMTItODM3Zi00ZGU2LTkxMmYtOTVjZTA5ODJkMzRjIiwiY2xpZW50X2lkIjoiNjczc2d1dmVtOWZycXZ2ZjFoN2xiOGI2OTQifQ.e5LdP0j3bPGAZYz0WGRFB0W_aC1rtGYd1wsHEbarB2XvT8sNqkmRTCs9gRxfyPfcmJ21ynJFEQ31W8NlG4v39O4SszlKTgkYZmGvZjkmXescsadQId7y6D4r1j54mu6QcWPof-ApKFwViHBDMcVa_aNZ6BvHEgY8WvvpZJ0a_R_8G7e3AMbMUeyw-P9PqCJTVcrOLKgisVacTJdIv70OGkumjWk6y3Puwv-Ujl-1l4CaydMEY7lkAWfbH4fgakrtVOBnMdpuIYhRfh75Q-5LY06AEiBLlS1u_6t606xAqdc1cN3osFbXhDdjsrmDUFBs7eFSyx_d0YhVP-9wuprNTQ\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d5830-b3bd-4778-a707-a08c83a358c8",
   "metadata": {},
   "source": [
    "## 5. High-Level Pipeline Flow\n",
    "\n",
    "For each vehicle and camera:\n",
    "\n",
    "1. Iterate over timestamps in 30-second increments\n",
    "2. Request a video clip using `/v1/video`\n",
    "3. If successful, retrieve the associated filename\n",
    "4. Poll `/v1/download` until the clip is ready\n",
    "5. Download the ROS bag file\n",
    "6. Save it into a size-limited bucket folder\n",
    "7. Repeat until the desired time window is covered\n",
    "\n",
    "This approach allows the pipeline to recover from missing data and transient API failures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532581a-bc79-46c7-a54f-7a4f7a7b9029",
   "metadata": {},
   "source": [
    "## 6. Directory Structure and File Management\n",
    "\n",
    "Downloaded files are organized as: `downloads/vehicle_name/download1/`\n",
    "\n",
    "Each `downloadN` folder:\n",
    "- Is capped at **10 GB**\n",
    "- Automatically rolls over when the size limit is reached\n",
    "\n",
    "Final archived data is later regrouped into:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5013dbc-3d73-49e1-8965-fff289c4e02e",
   "metadata": {},
   "source": [
    "YYYY-MM-DD/\n",
    "├── vehicle_camera_timestamp_30s.bag\n",
    "├── vehicle_camera_timestamp_30s.bag\n",
    "└── ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72ae66-b204-459a-b764-0d4c975b7246",
   "metadata": {},
   "source": [
    "## 7. Requesting Video Clips\n",
    "\n",
    "Video availability is queried using the `/v1/video` endpoint.\n",
    "\n",
    "Each request specifies:\n",
    "- Vehicle name\n",
    "- Camera name\n",
    "- Start timestamp (UTC)\n",
    "- End timestamp (start + 30 seconds)\n",
    "\n",
    "If the API returns a valid filename, the clip can be downloaded.\n",
    "Otherwise, the timestamp is treated as missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5873d9-9e7a-4c83-81de-dc1649552542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_video_filename(token, vehicle, camera, start_ts, duration_s=DURATION_SECONDS):\n",
    "    end_ts = start_ts + duration_s\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"vehicle\": vehicle, \"camera\": camera, \"startTime\": start_ts, \"endTime\": end_ts}\n",
    "    try:\n",
    "        r = requests.get(VIDEO_URL, headers=headers, params=params, timeout=(10, 120))\n",
    "    except (requests.ReadTimeout, requests.ConnectionError) as e:\n",
    "        when = dt.datetime.fromtimestamp(start_ts, dt.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[MISS][timeout] {when}Z {vehicle} {camera}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    code = r.status_code\n",
    "    if code in (400, 404):\n",
    "        body = (r.text or \"\").strip().replace(\"\\n\", \" \")\n",
    "        when = dt.datetime.fromtimestamp(start_ts, dt.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{r.status_code}] {when}Z {vehicle} {camera}: {body[:160]}\")\n",
    "        \n",
    "        return None, code\n",
    "    \n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        when = dt.datetime.fromtimestamp(start_ts, dt.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{r.status_code}] {when}Z {vehicle} {camera}: {e}\")\n",
    "        return None, code\n",
    "\n",
    "    try:\n",
    "        data = r.json()\n",
    "        if isinstance(data, dict) and \"filename\" in data:\n",
    "            return data[\"filename\"], 200\n",
    "        if isinstance(data, str) and data.endswith(\".bag\"):\n",
    "            return data, 200\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    txt = (r.text or \"\").strip()\n",
    "    if txt.endswith(\".bag\"):\n",
    "        return txt, 200\n",
    "    \n",
    "    ctype = r.headers.get(\"Content-Type\", \"\")\n",
    "    snippet = (r.text or \"\")[:200].replace(\"\\n\", \" \")\n",
    "    print(f\"[WARN] Unexpected /v1/video body (type={ctype!r}): {snippet!r}\")\n",
    "    return None, code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc954416-ce1e-4f31-80d1-85c595fb9ce9",
   "metadata": {},
   "source": [
    "## 8. Downloading and Polling for Readiness\n",
    "\n",
    "Video clips are not immediately available after being requested.\n",
    "\n",
    "To handle this:\n",
    "- The pipeline polls the `/v1/download` endpoint\n",
    "- Exponential backoff is applied between retries\n",
    "- Both presigned URLs and binary streams are supported\n",
    "\n",
    "This ensures robustness against:\n",
    "- Backend processing delays\n",
    "- Temporary server errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe2dba1-75e9-4dcc-b8d0-e7a5b344e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_bag_ready(token, filename, max_wait_sec=100, base_sleep=3):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params  = {\"filename\": filename}\n",
    "    deadline = time.time() + max_wait_sec\n",
    "    sleep = base_sleep\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            r = requests.head(DL_URL, headers=headers, params=params, timeout=15)\n",
    "            code = r.status_code\n",
    "            if code == 405:  \n",
    "                r = requests.get(DL_URL, headers=headers, params=params, timeout=15, stream=False)\n",
    "                code = r.status_code\n",
    "        except requests.RequestException:\n",
    "            code = None\n",
    "\n",
    "        if code == 200:\n",
    "            return True\n",
    "        if code in (404, 500, 503) or code is None:\n",
    "            time.sleep(sleep)\n",
    "            sleep = min(sleep * 1.5, 30)\n",
    "            continue\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        sleep = min(sleep * 1.5, 30)\n",
    "\n",
    "    return False\n",
    "\n",
    "def _get_presigned_or_binary_response(token, filename, timeout=180):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params  = {\"filename\": filename}\n",
    "    r = requests.get(DL_URL, headers=headers, params=params, stream=True, timeout=timeout, allow_redirects=False)\n",
    "\n",
    "    if r.is_redirect or r.status_code in (302, 303, 307, 308):\n",
    "        loc = r.headers.get(\"Location\")\n",
    "        if loc and loc.startswith((\"http://\", \"https://\")):\n",
    "            r.close()\n",
    "            return (\"url\", loc)\n",
    "\n",
    "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    if r.status_code == 200 and (\"octet-stream\" in ctype or \"application/x-rosbag\" in ctype or \"binary\" in ctype):\n",
    "        prefix = r.raw.read(4096, decode_content=True)\n",
    "        if _looks_like_url_bytes(prefix):\n",
    "            r.close()\n",
    "            presigned = prefix.decode(\"utf-8\", errors=\"ignore\").strip().split()[0]\n",
    "            return (\"url\", presigned)\n",
    "        r.close()\n",
    "        r2 = requests.get(DL_URL, headers=headers, params=params, stream=True, timeout=timeout, allow_redirects=True)\n",
    "        return (\"resp\", r2)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        body = r.content[:8192]\n",
    "        txt = body.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        if txt.startswith((\"http://\", \"https://\")):\n",
    "            r.close()\n",
    "            return (\"url\", txt.split()[0])\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "            if isinstance(data, dict) and \"url\" in data:\n",
    "                r.close()\n",
    "                return (\"url\", data[\"url\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    r.close()\n",
    "    return (\"fail\", f\"status={r.status_code}, ctype={ctype}\")\n",
    "\n",
    "def download_bag(token, filename, out_dir=OUTPUT_DIR, final_filename: str | None = None):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not wait_for_bag_ready(token, filename, max_wait_sec=120, base_sleep=3):\n",
    "        return False\n",
    "\n",
    "    kind, payload = _get_presigned_or_binary_response(token, filename, timeout=180)\n",
    "\n",
    "    chosen_name = final_filename if final_filename else filename\n",
    "    out_path = out_dir / chosen_name\n",
    "\n",
    "    if kind == \"url\":\n",
    "        return _download_stream(payload, out_path, timeout=300, headers=None)\n",
    "\n",
    "    if kind == \"resp\":\n",
    "        resp = payload\n",
    "        if final_filename is None:\n",
    "            suggested = _extract_filename_from_headers(resp, filename)\n",
    "            out_path = out_dir / suggested\n",
    "\n",
    "        tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in resp.itercontent(1 << 20):\n",
    "                if chunk:\n",
    "                    if len(chunk) < 1024 and _looks_like_url_bytes(chunk.strip()):\n",
    "                        presigned = chunk.decode(\"utf-8\", errors=\"ignore\").strip().split()[0]\n",
    "                        resp.close()\n",
    "                        f.close()\n",
    "                        try: tmp.unlink(missing_ok=True)\n",
    "                        except Exception: pass\n",
    "                        return _download_stream(presigned, out_path, timeout=300, headers=None)\n",
    "                    f.write(chunk)\n",
    "        resp.close()\n",
    "        tmp.replace(out_path)\n",
    "        return out_path\n",
    "\n",
    "    print(f\"[WARN] failed to download {filename}: {payload}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa89cd-7256-4a7a-9a9d-e8be69362323",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Reliability Strategies\n",
    "\n",
    "Several failure modes were observed during development:\n",
    "\n",
    "### 404 Errors\n",
    "- Occur when no video exists for a timestamp\n",
    "- Solution: skip ahead by **5 minutes** to avoid dense retry loops\n",
    "\n",
    "### 5xx Errors\n",
    "- Transient server-side failures\n",
    "- Solution: exponential backoff with retries\n",
    "\n",
    "### Rate Limiting\n",
    "- Rapid requests can trigger throttling\n",
    "- Solution: fixed delay (`THROTTLE_SEC = 0.2`) and randomized retries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e921465-4dfe-4167-9634-c31074fe5ebe",
   "metadata": {},
   "source": [
    "## 10. Timezone Handling\n",
    "\n",
    "All API timestamps are in **UTC**, while data organization was initially based on local time.\n",
    "\n",
    "This caused misalignment when grouping clips by day.\n",
    "\n",
    "Solution:\n",
    "- Explicit timezone conversion using Python's `ZoneInfo`\n",
    "- Folder boundaries are defined consistently using UTC or local time as needed\n",
    "\n",
    "This eliminated off-by-one-day errors in archived data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb813e87-d1e0-42cd-bed9-01395bc05ac5",
   "metadata": {},
   "source": [
    "## 11. Full-Day and Windowed Downloads\n",
    "\n",
    "The pipeline supports multiple retrieval modes:\n",
    "- Full-day downloads\n",
    "- Fixed-hour windows\n",
    "- Random sampling\n",
    "- Availability scanning without downloading\n",
    "\n",
    "This flexibility allows:\n",
    "- Efficient debugging\n",
    "- Partial reprocessing\n",
    "- Parallel execution across vehicles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10b2f6c-eba0-419c-9d5c-16a14d8dc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_whole_day(token: str, vehicles, camera: str,\n",
    "                   year: int, month: int, day: int,\n",
    "                   max_files_per_vehicle: int | None = None):\n",
    "    if isinstance(vehicles, str):\n",
    "        vehicles = [vehicles]\n",
    "\n",
    "    start_ts, end_ts = day_bounds_utc(year, month, day)\n",
    "    total_seconds = end_ts - start_ts\n",
    "    today_start = int(dt.datetime.now(dt.UTC).replace(hour=0, minute=0, second=0, microsecond=0).timestamp())\n",
    "    if start_ts >= today_start:\n",
    "        raise ValueError(\"That day is 'today' in UTC. The batch API requires using the realtime API for the current day.\")\n",
    "\n",
    "    cap = max_files_per_vehicle\n",
    "    print(f\"\\n=== WHOLE DAY {year:04d}-{month:02d}-{day:02d} UTC ===\")\n",
    "    print(f\"Window: {dt.datetime.fromtimestamp(start_ts, dt.UTC):%Y-%m-%d %H:%M:%S}Z\"\n",
    "          f\" → {dt.datetime.fromtimestamp(end_ts, dt.UTC):%Y-%m-%d %H:%M:%S}Z\")\n",
    "\n",
    "    stats = {}\n",
    "    for vehicle in vehicles:\n",
    "        saved = 0\n",
    "        ts = start_ts\n",
    "        stats[vehicle] = {\"saved\": 0, \"miss\": 0, \"skip\": 0}\n",
    "        while ts < end_ts and (cap is None or saved < cap):\n",
    "            fname, code = request_video_filename(token, vehicle, camera, ts, DURATION_SECONDS)\n",
    "            when = dt.datetime.fromtimestamp(ts, dt.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            if fname:\n",
    "                day_str = dt.datetime.fromtimestamp(ts, DAY_TZ).strftime(\"%Y-%m-%d\")\n",
    "                out_dir = out_dir_for(vehicle, day_str)\n",
    "                final_name = timeslot_filename(vehicle, camera, ts, DURATION_SECONDS)\n",
    "                path = download_bag(token, fname, out_dir, final_filename=final_name)\n",
    "\n",
    "                if not path:\n",
    "                    stats[vehicle][\"skip\"] += 1\n",
    "                    print(f\"[SKIP] {when}Z {vehicle}: download failed\")\n",
    "                else:\n",
    "                    sz = path.stat().st_size\n",
    "                    if sz < MIN_VALID_BYTES:\n",
    "                        try: path.unlink()\n",
    "                        except Exception: pass\n",
    "                        stats[vehicle][\"skip\"] += 1\n",
    "                        print(f\"[WARN] {when}Z {vehicle} tiny file ({sz} bytes) – counted as SKIP\")\n",
    "                    else:\n",
    "                        saved += 1\n",
    "                        stats[vehicle][\"saved\"] += 1\n",
    "                        cap_str = f\" ({saved}/{cap})\" if cap else \"\"\n",
    "                        print(f\"[OK]  {when}Z -> {_rel_out(path)} (size={sz/1e6:.1f} MB){cap_str}\")\n",
    "            else:\n",
    "                stats[vehicle][\"miss\"] += 1\n",
    "                print(f\"[MISS] {when}Z {vehicle} (code={code})\")\n",
    "\n",
    "            ts += DURATION_SECONDS\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e022c-6e69-4f20-92cb-848a5c344081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae6a2a-0c14-43b8-8e0a-9623d50fd836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
